{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16bf69c-3d1f-4ba4-b0bf-0ab821100297",
   "metadata": {},
   "source": [
    "# Neural network for track classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d53181-5a74-433b-bbd0-7a731b7761d3",
   "metadata": {},
   "source": [
    "Tracks detected in particle colliders like the LHC can originate from various sources, such as:\n",
    "\n",
    "- Prompt tracks: Directly from the collision point.\n",
    "- Pile-up tracks: From additional, unwanted collisions.\n",
    "- Non-prompt tracks: From decays of B and D mesons and other hadrons.\n",
    "\n",
    "Understanding the origin of these tracks is crucial for many physics analyses and will help in creating a powerful lepton isolation algorithm.\n",
    "In this notebook, we aim to classify these tracks using machine learning techniques.\n",
    "\n",
    "**Objective:** Our main objective is to build a classification algorithm that can accurately determine the origin of each track. We will:\n",
    "\n",
    "- Use track information (impact parameters, transverse momentum, relative momentum w.r.t. closest lepton) to predict their origin.\n",
    "- Implement a fully connected neural network using PyTorch, a leading library for building deep learning models.\n",
    "\n",
    "**Tools:** To achieve our goals, we will make use of the following tools and libraries:\n",
    "\n",
    "- PyTorch: For designing and training the neural network.\n",
    "- scikit-learn (sklearn): To preprocess the data and for implementing other useful machine learning utilities.\n",
    "- scikit-plot: For visualizing the results, especially for metrics like the ROC curve.\n",
    "- atlas-ftag-tools Python package: Specifically designed for handling track data in the context of the ATLAS experiment.\n",
    "\n",
    "Eventually, you can use the output of the network to build a track-based isolation variable (e.g. by computing a per-track discriminant and summing over the discriminants of all tracks inside a cone)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c291208f-b9cd-4423-bc1d-3052d8b0e57a",
   "metadata": {},
   "source": [
    "## Import of python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d301148-9fd6-4320-8439-165d0c7809e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ftag.hdf5 import H5Reader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a10650-74e4-4d92-b953-c0370dbde56b",
   "metadata": {},
   "source": [
    "## Preparation: Getting the `tracks` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054df1c-9744-44d4-b168-de3af4d87f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"maya.h5\"\n",
    "reader = H5Reader(fname, batch_size=100, jets_name=\"muons\")\n",
    "\n",
    "muon_variables = [\"pt\", \"eta\", \"ptvarcone30Rel\", \"iffClass\"]\n",
    "track_variables = [\n",
    "    \"pt\", \"eta\", \"phi\", \n",
    "    \"ptfrac\", \"dr_trackjet\", \"dr_leptontrack\", \n",
    "    \"btagIp_d0\", \"btagIp_z0SinTheta\",\n",
    "    \"btagIp_d0_significance\", \"btagIp_z0SinTheta_significance\", \n",
    "    \"leptonID\",\n",
    "    \"ftagTruthOriginLabel\", \"ftagTruthTypeLabel\", \"ftagTruthVertexIndex\",\n",
    "    \"valid\"\n",
    "]\n",
    "data = reader.load({\"muons\": muon_variables, \"muon_tracks\": track_variables}, num_jets=3_000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6bf5d-3759-43a6-9e38-71322ae7e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tracks dataset\n",
    "tracks = data['muon_tracks']\n",
    "tracks = tracks.flatten()\n",
    "tracks = tracks[np.where(tracks[\"valid\"])]\n",
    "df = pd.DataFrame(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6817e9c-1012-4ea2-8a5b-3e2ba9d32920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature matrix X and label vector y\n",
    "X = df.drop(columns=[\"valid\", \"ftagTruthOriginLabel\", \"ftagTruthTypeLabel\", \"ftagTruthVertexIndex\"])\n",
    "y = df[[\"ftagTruthOriginLabel\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15f4e2-ab76-421d-91a0-7f78b07988f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the feature matrix\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd93991-07b0-4c51-bb31-dba6f8226143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the target labels\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552701c6-1964-40ad-b7b5-3567e00168a1",
   "metadata": {},
   "source": [
    "## Exploratory Analysis: Correlation of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab383ab-8192-45d9-92d7-5ee002e60d94",
   "metadata": {},
   "source": [
    "Before training neural networks (or any machine learning model), it makes sense to conduct an exploratory analysis of the dataset. Otherwise you just blindly trust a non-deterministic optimisation procedure, which is rarely a wise strategy.\n",
    "\n",
    "One component of this analysis is examining the correlation between features. It helps you identify which features carry similar information and how they relate to each other, possibly introducing redundancy in your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e82ffb-bbb8-407b-976c-1089826fda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 7))\n",
    "corr_X = X.corr()\n",
    "\n",
    "# plot correlations among variables as heatmap\n",
    "ax = sns.heatmap(\n",
    "    corr_X,\n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True,  annot=True, fmt='.2f', ax = ax)\n",
    "ax.set_title(\"Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72900213-46b7-48b6-8b47-62c60fa2adb1",
   "metadata": {},
   "source": [
    "## Preprocessing of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f356e-e3ea-415a-8f8d-20ea077ca51f",
   "metadata": {},
   "source": [
    "When working with numerical data in machine learning, it is crucial to scale features to a common scale. This ensures that no single feature dominates others in the model training process, leading to more stable and meaningful results. The `StandardScaler` from `sklearn.preprocessing` is a popular choice for this purpose as it scales the data such that the mean is centred as zero and scales the data to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e0bb7-a58b-410e-b688-007a3a13dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "\n",
    "scaled_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293b379-5414-4a72-af55-bc35ac75035b",
   "metadata": {},
   "source": [
    "## Dataset for pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959c092-9714-4a8d-b506-a46a23d3298f",
   "metadata": {},
   "source": [
    "We will make use of pytorch for training. We will provide the data as a torch dataset.\n",
    "To provide some additional functionality, we define our own dataloader class based on the torch Dataset class. This dataset class puts together everything of the above (except for the exploratory analysis) and bundles it, so that you just have to define a `TrackDataset` and under its hood all the hard work is done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d1e5e-4fdc-4058-b005-29565dac64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackDataset(Dataset):\n",
    "    def __init__(self, file_path=\"maya.h5\", num_jets=3000):\n",
    "        super().__init__()\n",
    "        reader = H5Reader(file_path, batch_size=100, jets_name=\"muons\")\n",
    "\n",
    "        muon_variables = [\"pt\", \"eta\", \"ptvarcone30Rel\", \"iffClass\"]\n",
    "        track_variables = [\n",
    "            \"pt\", \"eta\", \"phi\", \n",
    "            \"ptfrac\", \"dr_trackjet\", \"dr_leptontrack\", \n",
    "            \"btagIp_d0\", \"btagIp_z0SinTheta\",\n",
    "            \"btagIp_d0_significance\", \"btagIp_z0SinTheta_significance\", \n",
    "            \"leptonID\",\n",
    "            \"ftagTruthOriginLabel\", \"ftagTruthTypeLabel\", \"ftagTruthVertexIndex\",\n",
    "            \"valid\"\n",
    "        ]\n",
    "        data = reader.load({\"muons\": muon_variables, \"muon_tracks\": track_variables}, num_jets=num_jets)\n",
    "        # get tracks dataset\n",
    "        tracks = data['muon_tracks']\n",
    "        tracks = tracks.flatten()\n",
    "        tracks = tracks[np.where(tracks[\"valid\"])]\n",
    "        df = pd.DataFrame(tracks)\n",
    "        df.drop(columns=[\"valid\"], inplace=True)\n",
    "\n",
    "        # split into classes\n",
    "        df_prompt = df[df['ftagTruthOriginLabel'] == 2]\n",
    "        df_pileup = df[df['ftagTruthOriginLabel'] == 0]\n",
    "        df_nonprompt = df[(df['ftagTruthOriginLabel'] != 0) & (df['ftagTruthOriginLabel'] != 2)]\n",
    "\n",
    "        # create a balanced dataset\n",
    "        n_events = min([len(df_prompt), len(df_pileup), len(df_nonprompt)])\n",
    "        df_prompt = df_prompt.sample(n=n_events, random_state=42)\n",
    "        df_pileup = df_pileup.sample(n=n_events, random_state=42)\n",
    "        df_nonprompt = df_nonprompt.sample(n=n_events, random_state=42)\n",
    "\n",
    "        # merge and shuffle\n",
    "        df = pd.concat([df_prompt, df_pileup, df_nonprompt])\n",
    "        df = df.sample(frac=1, random_state=42)\n",
    "        \n",
    "        # store the inputs and outputs as values\n",
    "        X = df.drop(columns=[\"ftagTruthOriginLabel\", \"ftagTruthTypeLabel\", \"ftagTruthVertexIndex\"]).values\n",
    "        # y = df[\"ftagTruthOriginLabel\"].values\n",
    "\n",
    "        # convert y to a three column dataset for one-hot encoding\n",
    "        is_prompt = df['ftagTruthOriginLabel'] == 2\n",
    "        is_pileup = df['ftagTruthOriginLabel'] == 0\n",
    "        \n",
    "        df['is_prompt'] = is_prompt.astype(int)\n",
    "        df['is_pileup'] = is_pileup.astype(int)\n",
    "        df['is_other'] = (~(is_prompt | is_pileup)).astype(int)\n",
    "\n",
    "        target_columns = ['is_prompt', 'is_pileup', 'is_other']\n",
    "        y = df[target_columns].values\n",
    "        \n",
    "        # standardise X\n",
    "        scale = StandardScaler()\n",
    "        scaled_X = scale.fit_transform(X)\n",
    "        \n",
    "        # make tensor\n",
    "        self.X = torch.tensor(scaled_X, dtype=torch.float32)\n",
    "        self.y = torch.squeeze(torch.tensor(y, dtype=torch.float32))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    def get_splits(self, n_test=0.1, n_val=0.1):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        val_size = round(n_val * len(self.X))\n",
    "        train_size = len(self.X) - test_size - val_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f95e9d-84d9-4446-8660-a3592b56a5c6",
   "metadata": {},
   "source": [
    "To simplify our lives, we will use a class to provide the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827892d-ed1c-45c0-ba78-0fceab3d532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrackDataset(\"maya.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec28fd-a6a5-4892-bbde-a2c641d345a5",
   "metadata": {},
   "source": [
    "We can now inspect the dataset and observe that it contains the feature matrix and the target labels. Finally we check if the dataset is balanced, i.e. if every class is equally populated (which we have implemented in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d12a8-e747-463c-9e57-31b0d87517fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aad12-8c30-47f4-acf7-276792f3059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f303ba-60d2-4cd6-a646-d96466020148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the dataset really is balanced\n",
    "dataset.y.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4945c6b-3ac7-4219-b3a9-a3e04454fd7a",
   "metadata": {},
   "source": [
    "With the `TrackDataset` in place, we now define a function to load the train, validation and test datasets.\n",
    "\n",
    "We will use the validation dataset to monitor the training, while the final evaluation of the performance will be done using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273da69d-e603-4464-bfcc-6095b23e6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all the random seeds\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "# define function for providing train, validation and test dataloaders\n",
    "def prepare_data():\n",
    "    # load the dataset\n",
    "    dataset = TrackDataset()\n",
    "    print(dataset.__len__)\n",
    "\n",
    "    # calculate split\n",
    "    train, val, test = dataset.get_splits()\n",
    "\n",
    "    # prepare data loaders for training\n",
    "    train_dl = DataLoader(train, batch_size=100, shuffle=True,  worker_init_fn=seed_worker,\n",
    "    generator=g)\n",
    "    val_dl = DataLoader(val, batch_size=100, shuffle=False)\n",
    "    test_dl = DataLoader(test, batch_size=100, shuffle=False,  worker_init_fn=seed_worker,\n",
    "    generator=g)\n",
    "    return train_dl,  val_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791d848-5c6f-4174-8dc7-92c8b1eccc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a01a3-0794-44a6-a662-3ad3941ee31d",
   "metadata": {},
   "source": [
    "## Defining the machine learning model: fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0d7c6-71f8-4926-b68b-510d22305b8a",
   "metadata": {},
   "source": [
    "In this section, we will define the architecture of our machine learning model, which is a fully connected (dense) neural network. Fully connected networks are a type of deep learning model where each neuron in one layer is connected to all neurons in the next layer. This connectivity pattern makes them particularly suited for learning from structured data like the track features in our dataset.\n",
    "\n",
    "**Model Structure**\n",
    "\n",
    "Our model, `NNClassifier`, allows for setting the number of hidden layers and the size of each layer when defining it. Here’s a brief outline of its components:\n",
    "\n",
    "- Input Layer: The first layer in the network, which receives the input features. The number of neurons in this layer matches the dimensionality of our input data (default: 11 features).\n",
    "- Hidden Layers: These layers allow the network to learn more complex patterns in the data. Each hidden layer is followed by a `ReLU` activation function, which introduces non-linearity into the model, enabling it to capture more complex relationships between the inputs and outputs. The use of multiple hidden layers helps in deep representation learning, which is important for accurately classifying the track data.\n",
    "- Output Layer: The final layer of our model has three neurons, corresponding to the three classes of track origins (prompt, pile-up, and other). It uses a `Softmax` activation function to output a probability distribution over the three classes. This setup ensures that each output can be interpreted as the probability that a given input belongs to one of the three classes.\n",
    "- Forward Pass: Defines how the input data flows through these layers. Each layer's output becomes the next layer's input, culminating in the output layer that produces the final prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce63c36-8a11-4c06-ad10-07e62ed0f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23080662-0763-4d42-ab66-c0947a0a0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, hidden_layer_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Assume hidden_layer_size includes input to first hidden layer and all subsequent layer sizes\n",
    "        # Add the input layer\n",
    "        self.layers.append(nn.Linear(hidden_layer_size[0][0], hidden_layer_size[0][1]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i in range(1, num_hidden_layers - 1):  # Adjusted to properly index hidden_layer_size\n",
    "            self.layers.append(nn.Linear(hidden_layer_size[i][0], hidden_layer_size[i][1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        # Add the output layer for three classes\n",
    "        self.layers.append(nn.Linear(hidden_layer_size[-2][1], 3))  # Output size set to 3 for three classes\n",
    "        self.layers.append(nn.Softmax(dim=1))  # Use Softmax for multi-class classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981737a-56ac-452b-925f-36e09f26907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters and structure of network\n",
    "num_hidden_layers = 6\n",
    "num_features = len(dataset.X.T) # number of columns in X\n",
    "\n",
    "# you can try different configurations of the network, e.g. adding or removing layers\n",
    "hidden_layer_size = [(num_features, 20), (20, 40), (40, 80), (80, 60), (60, 20), (20,3)]\n",
    "\n",
    "# this is the network\n",
    "model = NNClassifier(num_hidden_layers, hidden_layer_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41d957-5f49-4558-8b65-5faae721bc96",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be91e0-9d71-44d2-aa2b-e9129ab4dc33",
   "metadata": {},
   "source": [
    "Training a model in machine learning involves iteratively adjusting the model's parameters to minimize the difference between the predicted output and the actual output.\n",
    "This process requires a suitably defined machine learning model (here: fully connected neural network) but data loaders to ensure efficient data flow, as well as metrics for optimizing parameters, and assessing model performance.\n",
    "\n",
    "**Setting Up the Training Loop**\n",
    "\n",
    "We structure our training function to handle both training and validation phases within each epoch. This approach allows us to monitor the model's performance on unseen data (validation set) while it learns from the training set. The training loop has the following components:\n",
    "\n",
    "- Model Transfer to Device: We ensure that the model operates on the correct device (CPU or GPU). If you train large datasets on CPU, you will have a bad (and long) time.\n",
    "- Loss and Accuracy Tracking: We maintain a history dictionary to track training and validation loss and accuracy. This data will help us evaluating the model's learning progression and tuning its hyperparameters.\n",
    "- Batch Processing: The model processes data in batches, which helps to balance computational efficiency and memory usage. Batch processing is also beneficial for gradient descent optimization, as it introduces some noise into the gradient calculations, potentially helping to avoid local minima which are harmful if the training gets stuck in these.\n",
    "- Forward and Backward Passes: For each batch, the model performs a forward pass to compute predictions and a backward pass to update weights based on the gradient of the loss function.\n",
    "- Optimizer Step: Post the backward pass, the optimizer updates the model parameters based on the gradients computed during backpropagation.\n",
    "- Validation Phase: After updating the model on the entire training data, we evaluate its performance on the validation set without making any further adjustments to the model's parameters.\n",
    "\n",
    "\n",
    "**Epoch-wise Iteration**\n",
    "\n",
    "The training process is divided into epochs. An epoch represents one full cycle through the entire training dataset. Here’s what happens during each epoch:\n",
    "\n",
    "- Training Phase: The model learns from the training data, adjusting its weights to minimize the loss function.\n",
    "- Validation Phase: The model's performance is assessed on a separate validation dataset that it has not seen during training. This helps estimating how well the model is likely to perform on general, unseen data.\n",
    "\n",
    "**Performance Metrics**\n",
    "\n",
    "We calculate and log the average loss and accuracy for both training and validation datasets after each epoch. We also track the time taken to complete the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97973715-844b-462f-88f1-c2f96397161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0bdbb4-097e-4bfb-a0e1-11ff97688835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_dl, val_dl, epochs=30, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    print(f'train() called: model={type(model).__name__}, optimizer={type(optimizer).__name__}(lr={optimizer.param_groups[0][\"lr\"]}), epochs={epochs}, device={device}')\n",
    "\n",
    "    # we will collect information about loss and accuracy on\n",
    "    # train and validation datasets per epoch in this dictionary\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "        'acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    # start the stopwatch and begin training loop\n",
    "    start_time_sec = time.time()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        # iterate over batches in training dataloader\n",
    "        for x, y in train_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(y, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += x.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / train_total\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_dl:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                loss = loss_fn(outputs, y)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(y, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += x.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Logging\n",
    "        history['loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    end_time_sec = time.time()\n",
    "    print('Training completed in: {} seconds'.format(end_time_sec - start_time_sec))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e676a-7e1d-4cce-b21a-ef14da77e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) \n",
    "loss_module = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108eef96-4af9-4885-90bd-393c6f7fb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(model=model, optimizer=optimizer, loss_fn=loss_module, train_dl=train_dl, val_dl=val_dl, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6dfda6-4eb0-410d-a51e-9fdd9e7783e4",
   "metadata": {},
   "source": [
    "## Evaluation of trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ee841-92f2-4d58-b52b-b5a2c9e79fe8",
   "metadata": {},
   "source": [
    "After training a machine learning model, the next step is the evaluation of its performance. The following metrics are considered:\n",
    "\n",
    "- Accuracy: This is a primary metric for evaluating classification models. It gives us a straightforward percentage of correctly predicted instances out of the total number of instances evaluated.\n",
    "- Confusion Matrix: Provides a detailed breakdown of the model's performance across all classes, showing true positives, false positives, true negatives, and false negatives for each class. This matrix is particularly useful for identifying which classes are well-predicted by the model and which are often confused with others.\n",
    "- Receiver Operating Characteristic (ROC) Curve and Area Under Curve (AUC): These are helpful for assessing the performance of a classification model at various threshold settings. The ROC curve plots the true positive rate against the false positive rate at different threshold levels.\n",
    "\n",
    "**Evaluation Function Implementation**\n",
    "\n",
    "In our evaluation function eval_model, the following steps are implemented:\n",
    "\n",
    "- Model State: The model is set to evaluation mode, which disables dropout and batch normalization during the inference, ensuring consistent results across different evaluations.\n",
    "- Data Handling: Data from the loader is processed batch by batch without any gradient calculations (`torch.no_grad()`), which minimizes memory consumption and computational overhead.\n",
    "- Accuracy Calculation: As predictions are made, they are compared to the true labels to compute the overall accuracy of the model.\n",
    "- Confusion Matrix Visualization: Using `scikitplot` (you might need to install this), we visually represent how well the model has predicted across different classes, highlighting potential areas for improvement.\n",
    "- ROC and AUC Calculation: For each class, the ROC curve is calculated along with the AUC score. AUC provides a scalar value summarizing the overall ability of the model to discriminate between positive and negative classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e44e45-7b53-4d4d-8484-8cb219b70cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220c3ff-e21f-4ce1-be8d-aceb19ea501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "def eval_model(model, data_loader, device='cpu', class_count=3):\n",
    "    model.eval()  # Set model to eval mode\n",
    "    true_preds, num_preds = 0, 0\n",
    "    pred_list = []\n",
    "    target_list = []\n",
    "    outputs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data_inputs, data_target in data_loader:\n",
    "            inputs = data_inputs.to(device)\n",
    "            targets = data_target.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(inputs)\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            _, true_labels = torch.max(targets, 1)\n",
    "\n",
    "            # Collect for analysis\n",
    "            pred_list.extend(pred_labels.cpu().numpy())\n",
    "            target_list.extend(true_labels.cpu().numpy())\n",
    "            outputs_list.extend(outputs.cpu().numpy())\n",
    "\n",
    "            # Evaluate accuracy\n",
    "            true_preds += (pred_labels == true_labels).sum().item()\n",
    "            num_preds += targets.size(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = true_preds / num_preds * 100\n",
    "    print(f\"Accuracy of the model: {accuracy:.2f}%\")\n",
    "\n",
    "    # Generate confusion matrix plot\n",
    "    skplt.metrics.plot_confusion_matrix(target_list, pred_list, figsize=(8, 8))\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare data for ROC calculation\n",
    "    target_array = label_binarize(target_list, classes=list(range(class_count)))\n",
    "    outputs_array = torch.nn.functional.softmax(torch.tensor(outputs_list), dim=1).numpy()\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    colors = cycle(['blue', 'red', 'green', 'cyan', 'magenta', 'yellow', 'black'])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for i, color in zip(range(class_count), colors):\n",
    "        fpr[i], tpr[i], _ = roc_curve(target_array[:, i], outputs_array[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c0b99-fd3d-4256-b0a9-462f8d937697",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model, data_loader=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75708b38-7e5a-4520-8b11-1318808d5b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
